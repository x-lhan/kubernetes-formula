{% set version = "v1.7.0" %}

kubernetes:
  # version info
  version: {{ version }}
  kube-apiserver_docker_tag: {{ version }}
  kube-controller-manager_docker_tag: {{ version }}
  kube-scheduler_docker_tag: {{ version }}
  kube-proxy_docker_tag: {{ version }}
  flannel_docker_tag: v0.7.1-amd64
  
  # image info
  kube_docker_registry: "gcr.io/google_containers"
  hyperkube_image: gcr.io/google_containers/hyperkube
  flannel_image: "quay.io/coreos/flannel"

  # Automatic detection of a minion running in the AWS cloud
  {% if grains['uuid'].startswith('ec2') %}
  cloud: aws
  {% endif %}
  
  # determine support systemd or not
  {% if grains['oscodename'] in [ 'vivid', 'wily', 'jessie', 'xenial', 'yakkety' ] %}
  is_systemd: True
  systemd_system_path: /lib/systemd/system
  {% elif grains['os_family'] == 'RedHat' %}
  is_systemd: True
  systemd_system_path: /usr/lib/systemd/system
  {% else %}
  is_systemd: False
  {% endif %}
  
  # Recommand set of admission control plugins
  admission_control: "NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds"
  
  # Pod-to-pod network provider. support "cni" and "kubenet", defualt to "cni".
  network_provider: "cni"
  # actual cni provider to use when "network_provider" is "cni". support "canal" and "flannel", defualt to "canal"
  cni_provider: "canal"
  # Service CIDR; when changed please also change "master_extra_sans" and "dns_server" and regenerate cluster certificates
  service_cluster_ip_range: "10.254.0.0/16"
  # Cluster pod CIDR
  cluster_cidr: "10.150.0.0/16"
  # Host network interface that binded, when changed please make sure mine funcation "network.internal_ip" is also changed
  bind_iface: eth0
  flannel_backend_mode: vxlan
  allocate_node_cidrs: true
  
  # If true, allow containers to request privileged mode.
  allow_privileged: true
  
  # Default log level to warning, consider increase to level 2(info) or level 4(debug) for more detail logs
  log_level: "--v=0"
  

  # If true, override node hostname to use saltstack minion_id and make minion_id resolvable using /etc/hosts file on master node.
  # Please note: in AWS even though hostname is override the nodename is discovered using cloud meta-data instead.
  minion_id_as_hostname: false
  
  # kuberentes api_server configurations
  api_server:
    port: 6443
    # when changed, please regenerate cluster certificates
    fqdn: kubernetes.api
    # If true, api_server will be schedulable like pool node. Default to false
    register_schedulable: false
    # If true, api_server will be able to exec/log like pool node using kubectl. Default to false
    debugging_handlers: false
  
  # certificates
  certs_files:
    - 'ca.crt'
    - 'server.key'
    - 'server.cert'
    - 'kubecfg.key'
    - 'kubecfg.crt'
  # Authentication
  admin:
    username: admin
  token:
    admin: DFvQ8GJ9JD4fKNfuyEddw3rjnFTkUKsv
    kube_proxy: DFvQ8GelB7afH3wClC9romaMPhquyyEe
    kubelet:  7bN5hJ9JD4fKjnFTkUKsvVNfuyEddw3r
  
  # addons
  addon_check_interval_sec: 300
  
  # storage class
  enable_default_storage_class: False
  default_storage_class_zones: ""
  
  # etcd
  etcd_install_base_url: "https://github.com/coreos/etcd/releases/download"
  etcd_version: 3.0.17
  # Etcd storage backend. support etcd2 and etcd3. Default to etcd3. 
  # Please note: etcd2 did not support snapshot feature.
  storage_backend: etcd3
  
  # If enabled, a kubernetes cronjob will be create to take etcd cluster snapshot on a single master node.
  # Required to define pillar data "etcd_snapshot_zone" (Pick any master node in the AZ)
  # or "etcd_snapshot_hostname" to narrow down which master host to run.
  # Required "storage_backend" to be "etcd3".(Since snapshot is V3 only feature)
  # Required "runtime_config" to include "batch/v2alpha1=true"
  enable_etcd_snapshot: False
  # (Optional) which storage_class this snapshot pvc related. Default to gp2
  # if did not override please enable_default_storage_class
  etcd_snapshot_storage_classname: gp2
  # (Optional) snapshot volume size. Default to 1Gi
  etcd_snapshot_disk_size: 1Gi
  # (Optional) snapshot taken cronjob defination. Default to daily run at 06:02AM UTC
  etcd_snapshot_schedule: "02 6 * * *"
  
  # dashboard
  enable_cluster_ui: True
  
  # rescheduler
  enable_rescheduler: True

  # dns
  enable_cluster_dns: True
  # the dns_server ip address is derived based on the service_cluster_ip_range. 
  # Due to this reliance, you can see how it's derived in the map.jinja file.
  # You can still manually set the dns_server ip address, make sure it's in the
  # service_cluster_ip_range
  # dns_server: 
  dns_domain: cluster.local
  federations_domain_map: ""

  # registry(require cloud provider support; see cluster_registry_storage_classname below)
  enable_cluster_registry: False
  # the registry_server ip address is derived based on the service_cluster_ip_range. 
  # Due to this reliance, you can see how it's derived in the map.jinja file.
  # You can still manually set the registry_server ip address, make sure it's in the
  # service_cluster_ip_range
  # registry_server: 
  cluster_registry_disk_size: 1Gi
  # (Optinal) which storage_class this registry pvc related. Default to gp2.
  # if did not override please enable_default_storage_class
  cluster_registry_storage_classname: gp2
